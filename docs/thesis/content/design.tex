\chapter{Design \label{design}}
This chapter focuses on the overall design of the solution, depicting the crucial parts. Firstly, the base system of the monitored environment. Secondly, container remotely controlled event-based monitoring of processes, file system changes and networking. Thirdly, deployment and emplacement of the isolated environment and additionally connecting it over network in a target facility or system.

\section{Specification \label{design:specs}}
This short section summarizes all specifications and assumptions considering the design. The target operating system is always Linux distribution Ubuntu 18.04.5+ with hardware enablement (HWE) or 20.04.x. The kernel specification is important for the machine hosting VMs to satisfy eBPF tools. In addition, the KVM (with QEMU) is used with the libvirt management library, which fully satisfies the choice of virtualization technology.

Regarding the VMs, there are no kernel specification, but they have Ubuntu 18.04.5+ or 20.04.x. Additionally, each VM has the minimum memory size of 4 GB and 2 virtual processors (vCPU). Although, this can vary depending of the host system resources.

\section{Environment architecture \label{design:env-arch}}
The environment has a solid underlying architecture considering a proper isolation layer and other prevention mechanisms to secure the hosting system. These operations and precautions are in compliance with the ISO 27002 standard. The bait environments within, are deliberately vulnerable in some way, therefore they are not designed to abide much of any of the ISO 27000 standards. The main goal is a mimicking production environment build atop of Kubernetes cluster. Sequentially, this section covers all in a bottom-up fashion through Kubernetes cluster design to system environments.

The considered specific control categories of the "Information technology -- Security techniques -- Code of practice for information security controls" - ISO 27002 are\footnote{All of the outlined categories are derived from the official standard \cite{iso:27002} with proper numbering preserved}:
\begin{enumerate}[label=\arabic*.,ref={ISO-27002 category \theenumi},start=12]
	\item Operations security \label{iso.12}
	\begin{enumerate}[label=\arabic*.,ref=\theenumi.\arabic*]
		\item Operational procedures and responsibilities \label{iso.12.1}
		\item Protection from malware \label{iso.12.2}
		\item Backup \label{iso.12.3}
		\item Logging and monitoring \label{iso.12.4}
		\item Control of operational software \label{iso.12.5}
	\end{enumerate}
	\item Communications security \label{iso.13}
	\begin{enumerate}[label=\arabic*.,ref=\theenumi.\arabic*]
		\item Network security management \label{iso.13.1}
	\end{enumerate}
	\item System acquisition, development and maintenance \label{iso.14}
	\begin{enumerate}[label=\arabic*.,ref=\theenumi.\arabic*,start=2]
		\item Security in development and support processes \label{iso.14.2}
	\end{enumerate}
\end{enumerate}

%12       
%- 12.1   X
%- 12.2  - slightly compliant on the host system, but not much can be done on a bait system
%- 12.3   X
%- 12.4  - all soft developed keeps a log file with metrics/data but also execution logs - monitoring with prometheus of basic linux metrics
%- 12.5  - mention semantic verionsing in monitoring (software used) + ansible like everything is pamametrized to be able to rollback
%13       X
%- 13.1   X
%14       X
%- 14.2   x

Before implementation, the whole solution including program code, configurations and operation process must be documented with proper backups in terms of these control categories. According to \ref{iso.12.1} part "12.1.1 Documented operating procedures", the solution must have a technical documentation of the operations, which means including installation, setup and configuration procedures whether they are automated or not. This category links to \ref{iso.14.2}, which discusses the security of version control systems and remote repositories used for sharing and archiving. All developed programs have a dedicated repository with no sensitive configurations, which could compromise the system. Last but not least, \ref{iso.12.3} refers to creating and maintaining proper backup procedures of valuable assets. Partly it correlates with the previous categories and only together function as secure, documented, archived and reproducible in case of any failure.

\subsection{Base system \label{design:env-arch:base}}
The lowest layer is an Ubuntu host machine with KVM virtual machines (VM). Deriving from a Kubernetes cluster design in production environment, which often isn't a single-node architecture, the base system must be a set of coexisting VMs. Choosing the right number of VMs with respect to the overall resource capacity is not in scope of this thesis. Nevertheless, there are three base VMs serving as the base of the deceiving system.

The main specification defines that all VMs are identical and configured for remote operations, have functional inter-VM communication and meet all requirements (e.g. kernel version, basic security settings, hardening). Creating multiple identical VMs can be achieved by preceding mechanisms (e.g. \textit{cloud-init}\footnote{https://github.com/canonical/cloud-init}), sharing the same image or by provisioning mechanisms (e.g. \textit{Vagrant}, \textit{Ansible}).

It depends on the implementation, but since these VMs have a static and simple setup, preceding is not necessary. This technique is fully configurable and must be automated or manually ran for each VM with deviating variables e.g. host name, IP address. This is not a complicated process, but a more suitable approach is using a shared pre-configured image combined with a dedicated tool - Vagrant for seamless provisioning and installation. KVM with libvirt and Vagrant create a "VM as code" concept efficient in provisioning, preceding and whole VM management.

% TODO: move to implementation part
% This is where Vagrant comes in place to effectively manage the creation of custom VMs. In addition to Vagrant there is also inter-VM networking and choice of virtualization provider.

\subsubsection*{Setup \label{design:env-arch:base:setup}}
Base system has several dependencies and requires a custom setup of networking and host machine altogether. As mention in \autoref{design:specs}, the host depends on QEMU, KVM and libvirt to run VMs. Additionally, the VMs are connected to a libvirt-managed management network (MGMT network) and a standard inter-VM network (NODE network) also simulating public IP address pool for the Kubernetes cluster. Both of these networks are represented as Linux network bridges and are segregated to comply with \ref{iso.13.1}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{base_system_design}
	\caption{Base system visualization. !!!!REVIEW ME!!!!}
	\label{image:design:base_system}
\end{figure}

Given that, the deceiving environment is isolated twice (Kubernetes-managed containers and VMs), the host machine is not expected to experience any malicious activity. Even though, a set of precautions is advised in case of any suspicious activity occurs otherwise. But only passive techniques are suitable, because all of activity from the isolated environment is routed through the host system, which must be allowed. Network security monitoring (NSM) solution designed for detection rather than prevention should effectively provide the sufficient visibility over the host system. Although, this is not the main thesis objective, so there are no specific requirements and it depends on the implementation.

\subsubsection*{Vagrant \label{design:env-arch:base:vagrant}}
Sharing a custom base image (Vagrant box), saves time on configuration and is less prone to error. Utilizing a pre-configured Vagrant box does not necessarily mean the configuration is immutable, all can be changed via Vagrant post-deploy commands and provisioning in the \textit{Vagrantfile}. Security-wise, creating a custom Vagrant box from the official Ubuntu image is recommended over publicly available Vagrant boxes from unverified owners. Not knowing the whole agenda of those boxes a full audit would be appropriate to approve their usage. Therefore, the all VMs have been created with a specific Vagrant box.

Each Vagrant box is a minimal Ubuntu (of satisfying version defined in \autoref{design:specs}) installation with the following configuration and setup:
\begin{itemize}
	\item 
	user setup including password protected \textit{root}
	\item 
	kernel parameters
	\begin{itemize}[label=$\hyphen$]
		\item
		enabled IP forwarding - \texttt{net.ipv4.ip\_forward}
	\end{itemize}
	\item 
	VM routing table entry to satisfy reverse path check - new route through the NODE network to the administration network or machine
	\item
	SSH daemon configuration
	\item 
	custom dependencies based on the implementation
\end{itemize}

Additionally everything else is done within the Vagrantfile, which is configurable with environment variables or other type of arguments. The input variables are the NODE network bridge name, IP prefix for the NODE network, number of nodes and vagrant box identifier. Altogether, the Vagrantfile creates all requested nodes as functional and remotely accessible VMs ready for Kubernetes installation and the deception environment configuration.

\subsection{Kubernetes cluster \label{design:env-arch:k8s}}
Kubernetes is complex and highly configurable, therefore a simple configuration is sufficient. There are many Kubernetes installation techniques and various derivatives meant for minimal setup and development (e.g. \textit{microk8s}\footnote{\url{https://microk8s.io/}}, \textit{minikube}\footnote{\url{https://minikube.sigs.k8s.io/docs/}}, \textit{k3s}\footnote{\url{https://k3s.io/}}). For this thesis a full Kubernetes ecosystem is preferred to mimic a production environment as much as possible.

\subsubsection*{Setup \label{design:env-arch:k8s:setup}}
Considering Kubernetes as a cloud-only platform, there are few differences to take into account when deploying microservices and applications on a bare-metal variant. Most importantly, Kubernetes is installed on those three base VMs in an arrangement of one master node with two worker nodes. Which briefly means, that the master node controls the cluster and does not provide any computational resources like the worker nodes do.

Things like storage and load balancing network traffic, are cloud provider services, which need to be substituted. Regarding persistent storage for demanding applications, the cluster is scaled up with additional dedicated node (data node) for storage. Data node is not part of the Kubernetes cluster, it serves as an external Network File System (NFS) server providing persistent storage. This node is setup in the same way as the other cluster nodes, except for the Kubernetes installation. Instead, setup with simple NFS server.

The Kubernetes ecosystem is installed via \textit{kubespray}\footnote{\url{https://kubespray.io/}}, which is a full Ansible skeleton for a complete cluster setup. Kubespray installs all dependencies, configures networking and assigns roles (master vs. worker) to all nodes. The cluster is ready for creating new objects, applications and environments.

Only after the cluster is fully functional, the before mentioned load balancing can be setup. For a bare-metal installation, the \textit{MetalLB}\footnote{\url{https://metallb.org/}} load balancer effective and provides full Kubernetes loadbalancer capabilities. MetalLB is setup in layer 2 mode, which refers to the data link layer of the RM OSI model. This mode utilizes the address discovery protocol (i.e. ARP) to route between nodes and turns one node into a point of enter. It is not a true load balancing technique, but it serves the key purpose of publishing common ports to the outside network.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{k8s_design}
	\caption{Desired system setup with an environment ready Kubernetes.}
	\label{image:design:k8s}
\end{figure}

\subsubsection*{Environments \label{design:env-arch:k8s:envs}}
%mention:
%- possible environment scenarios
%- desired structure (maybe belongs to implementation)
%	- using deployments over sole pods
%	- storage class over persistent volumes
%- most of what is in the k8s-environments repo but from a design point of view
%Practically ... about the environments from architecture POV
%Technically .. about git repos and proper management for redeploying
Each environment covers a whole system with multiple applications and subsystems. In terms of Kubernetes, the environment is a collection of compatible objects/resources. Some with simple functions and dedicated applications (e.g. FTP server, private Docker registry, Gitea/GitLab, mail server or Nextcloud) others with connected active processes or dependent applications (e.g. data pipelines or microservice ecosystem). 

Technically, an environment in terms of this thesis is a collection of YAML files stored in a remote repository easily deployed to the cluster. More specifically, Kubernetes \textit{pods}, \textit{services}, configuration (via e.g. \textit{configmaps}, \textit{secrets}), mounted volumes and/or permission-related objects. Practically, pods are replaced with \textit{deployments} and published services are using service type \textit{LoadBalancer} (utilizes MetalLB).

%TODO: maybe include a design of several environments only from the application POV e.g. scenario1 - FTP + mail + blog + gitlab, scenario2 - microservice application with multiple pods ...

\section{Container monitoring \label{design:con-mon}}
Monitoring a container means to effectively observe container file system, networking and process execution. Inspired by some related solutions, this section describes the monitoring mechanisms utilized in this thesis.

\begin{itemize}
	\item 
	Points of enter, such as honeypots that lure the threat actors to the environment. Could be local to the environment or remote anywhere in the Internet.
	\item 
	3 Ubuntu server nodes are the base to Kubernetes cluster holding and orchestrating the whole environment.
	\item 
	There are to be multiple environments.
	\item
	Any environment is automated and deployed to the cluster via Ansible playbooks.
	\item 
	The core monitoring tools and programs are deployed on the hosting nodes
\end{itemize}

\subsection{something}
- sneakpeek scripts but from a design point of view

\section{Deployment and emplacement \label{design:deployment}}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{abstract_diagram}
	\caption{Abstract idea of this thesis system emplacement.}
	\label{image:design:abstract}
\end{figure}